---
name: "ðŸ› ï¸ AWS S3 Publish Logs"
description: |
  This GitHub Action uploads logs to AWS S3.

inputs:
  aws_access_key_id:
    description: "Unique, public identifier for an AWS IAM user."
    required: true
    type: string
  aws_secret_access_key:
    description: "Long-term security credential for AWS account or an IAM user."
    required: true
    type: string
  aws_region:
    description: "AWS region for S3 bucket."
    required: true
    type: string
  s3_bucket:
    description: "Name of the Amazon S3 bucket."
    required: true
    default: "fdio-logs-s3-cloudfront-index"
    type: string
  s3_path:
    description: "Path within Amazon AWS S3 bucket."
    required: false
    default: "${{ github.job }}/${{ github.run_id }}"
    type: string
  archives_path:
    description: "Source directory with logs artifact to archive."
    required: false
    default: "${{ github.workspace }}/archives"
    type: string
  compress_files:
    description: "Compress files with gzip prior to upload"
    required: false
    default: true
    type: string

runs:
  using: "composite"
  steps:
    - name: Check if AWS CLI is pre-installed
      id: aws-binary-check
      shell: bash
      run: |
        if command -v aws >/dev/null 2>&1; then
            echo "AWS CLI is already installed. Skipping install."
            echo "AWS_CLI_PREINSTALLED=true" >> "$GITHUB_OUTPUT"
            aws --version
            exit 0
        fi
        echo "AWS_CLI_PREINSTALLED=false" >> "$GITHUB_OUTPUT"

    - name: Cache or Restore the zip
      if: ${{ steps.aws-binary-check.outputs.AWS_CLI_PREINSTALLED == 'false' }}
      uses: actions/cache@v4
      id: aws-cli-cache
      with:
        path: ${{ runner.temp }}/aws-cli-cache/*.zip
        key: ${{ runner.os }}-${{ runner.arch }}-aws-cli-v2-zip

    - name: Install AWS CLI
      shell: bash
      if: ${{ steps.aws-binary-check.outputs.AWS_CLI_PREINSTALLED == 'false' }}
      env:
        CACHE_HIT: ${{ steps.aws-cli-cache.outputs.cache-hit == 'true' }}
        CACHE_PATH: ${{ runner.temp }}/aws-cli-cache
      run: |
        AWS_PACKAGE="awscli-exe-linux-$(uname -m).zip"

        mkdir -p "$CACHE_PATH"
        cd "$CACHE_PATH"

        if [ "$CACHE_HIT" = "false" ]; then
            curl -fsSL \
                --retry 3 \
                --retry-delay 5 \
                --connect-timeout 15 \
                --max-time 60 \
                -o "$AWS_PACKAGE" "https://awscli.amazonaws.com/$AWS_PACKAGE"
        fi

        unzip -o -q "$AWS_PACKAGE"
        sudo ./aws/install --update
        rm -rf ./aws

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v5.0.0
      with:
        aws-access-key-id: ${{ inputs.aws_access_key_id }}
        aws-secret-access-key: ${{ inputs.aws_secret_access_key }}
        aws-region: ${{ inputs.aws_region }}

    - name: Upload archives directory content to S3
      shell: bash
      run: |
        get_content_type() {
          local file_ext="${1##*.}"
          case "$file_ext" in
            xml)  echo "application/xml" ;;
            html) echo "text/html" ;;
            txt)  echo "text/plain" ;;
            log)  echo "text/plain" ;;
            css)  echo "text/css" ;;
            md)  echo "text/markdown" ;;
            rst)  echo "text/x-rst" ;;
            csv)  echo "text/csv" ;;
            svg) echo "image/svg+xml" ;;
            jpg|jpeg) echo "image/jpeg" ;;
            png) echo "image/png" ;;
            gif) echo "image/gif" ;;
            js)   echo "application/javascript" ;;
            pdf) echo "application/pdf" ;;
            json) echo "application/json" ;;
            otf) echo "application/otf" ;;
            ttf) echo "application/ttf" ;;
            woff) echo "application/woff" ;;
            woff2) echo "application/woff2" ;;
            *)   echo "application/octet-stream" ;;
          esac
        }

        export -f get_content_type

        pushd "${{ inputs.archives_path }}"

        # Traverse and upload
        find . -type f | while read -r file; do
          tmp_file="$(mktemp).gz"              # temp gzip file
          if [[ "${{ inputs.compress_files }}" == "false" ]] || \
             [[ "${file}" == *.gz ]] ; then
            cp -a "${file}" "${tmp_file}"      # already compressed
            rel_path="${file#./}"              # relative path
            content_encoding=""
          else
            gzip -c "${file}" > "${tmp_file}"  # compress
            rel_path="${file#./}.gz"           # relative path
            content_encoding="gzip"
          fi

          content_type=$(get_content_type "${file%.gz}")

          S3_ARN="s3://${{ inputs.s3_bucket }}/vex-yul-rot-jenkins-1"
          S3_ARN="${S3_ARN}/${{ inputs.s3_path }}/${rel_path}"

          echo "Uploading ${rel_path} -> ${S3_ARN}"
          aws s3 cp "${tmp_file}" "${S3_ARN}" \
            --content-type "${content_type}" \
            ${content_encoding:+--content-encoding "${content_encoding}"}

          rm -f "${tmp_file}"
        done

    - name: Publish Link
      shell: bash
      # yamllint disable rule:line-length
      run: |
        echo "S3 Logs:"
        echo "::notice::https://logs.fd.io/vex-yul-rot-jenkins-1/${{ inputs.s3_path }}"
      # yamllint enable rule:line-length
